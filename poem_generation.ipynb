{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "############################################################\n",
        "#                ----   NLP    ----\n",
        "#           Generating Poems with TensorFlow and Keras\n",
        "#  Read more at: https://daehnhardt.com/blog/2022/07/11/tf-nlp\n",
        "#  Following the course at Udemy\n",
        "#  https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/\n",
        "#                ------------------\n",
        "############################################################\n",
        "import requests\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow.keras.utils as kerasutils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "######################################## Getting text corpus\n",
        "\n",
        "# usage:\n",
        "#           text, average_words_number=get_corpus(url=\"https://www.gutenberg.org/cache/epub/38572/pg38572.txt\",\n",
        "#                   get_part=True, start_phrase=\"LOVE SONNETS OF AN\",\n",
        "#                   end_phrase=\"_Now in Press_\" )\n",
        "def get_corpus(url, get_part=True, start_phrase=\"\", end_phrase=\"\"):\n",
        "    \"\"\"\n",
        "    Extracts text from a file located at the provided web address.\n",
        "    :param url: Link to the text file\n",
        "    :param get_part: when True, we get only text located between start_phrase and end_phrase strings\n",
        "    :param start_phrase:\n",
        "    :param end_phrase:\n",
        "    :return: a stripped text string without carriage returns, and the average number of words in line.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = requests.get(url).text\n",
        "    except:\n",
        "        print(\"Can not load the document at: \" + str(url))\n",
        "        return False\n",
        "\n",
        "    if get_part:\n",
        "        start = text.find(start_phrase)  # skip header\n",
        "        end = text.rfind(end_phrase)  # skip extra text at the end\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    # Split text on carriage returns\n",
        "    text = text.split('\\r')\n",
        "\n",
        "    # Strip off new lines and empty spaces from the text\n",
        "    text = [t.strip() for t in text]\n",
        "\n",
        "    average_number_of_words_in_line = round(sum([len(s.split()) for s in text]) / len(text))\n",
        "    return text, average_number_of_words_in_line\n",
        "\n",
        "\n",
        "######################################## Tokenizing text\n",
        "def create_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Returns tokenizer and total words number based on the extracted text.\n",
        "    :param text: a text corpus, extracted and preprocessed with get_corpus()\n",
        "    :return: tokenizer, total words number\n",
        "    \"\"\"\n",
        "    # Please note that I have removed symbols [.,;:] from the default filetr value\n",
        "    # This helps to preserve punctuation to a certain extent\n",
        "    tokenizer = Tokenizer(filters='\"#$%&()*+-/<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "\n",
        "    # Total number of words\n",
        "    vocabulary_length = len(tokenizer.word_index) + 1\n",
        "    return tokenizer, vocabulary_length\n",
        "\n",
        "\n",
        "######################################## Padding sequences\n",
        "\n",
        "def pack_sequences(text, tokenizer, total_words_number):\n",
        "  \"\"\"\n",
        "  Based on the corpus of documents and tokenizer, create padded sequences for further prediction task\n",
        "  :param corpus: Text strings\n",
        "  :param tokenizer: tokenizer\n",
        "  :param total_words_number: unique number of words in the corpus\n",
        "  :return: maximum length of sequences, predictors and labels\n",
        "  \"\"\"\n",
        "  # create input sequences using list of tokens\n",
        "  input_sequences = []\n",
        "  for line in text:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "  # pad sequences\n",
        "  max_sequence_len = max([len(x) for x in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "  # create predictors and labels\n",
        "  predictors, labels = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "  labels = kerasutils.to_categorical(labels, num_classes=total_words_number)\n",
        "  return max_sequence_len, predictors, labels\n",
        "\n",
        "\n",
        "######################################## Create Keras Sequential model with word embeddings\n",
        "def create_model(vocabulary_length, sequence_length):\n",
        "  model = Sequential()\n",
        "  model.add(\n",
        "        Embedding(input_dim=vocabulary_length, output_dim=100, input_length=sequence_length - 1))\n",
        "  model.add(Bidirectional(LSTM(150, return_sequences=False))) \n",
        "  model.add(Dense(vocabulary_length, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def write_poem(model, tokenizer, max_sequence_length, seed_text=\"The Moon and Sun\", next_words=6, paragraphs=3):\n",
        "    \"\"\"\n",
        "    Uses fitted text generating Keras Sequential model to write a poem.\n",
        "    :param model: Keras sequential model, fitted to a text corpus\n",
        "    :param tokenizer: Tokenizer\n",
        "    :param max_sequence_length: Maximum length of text sequences\n",
        "    :param seed_text: a text sring to start poem generation\n",
        "    :param next_words: Number of words in a sentence\n",
        "    :param paragraphs: Number of paragraphs in the generated poem\n",
        "    :return: text of the generated poem\n",
        "    \"\"\"\n",
        "    poem = seed_text.capitalize() + \"\\n\\n\"\n",
        "    while paragraphs > 0:\n",
        "        paragraph = \"\"\n",
        "        for word_number in range(next_words):\n",
        "            sentence = \"\\n\"\n",
        "            for _ in range(next_words):\n",
        "                token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "                token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
        "                predicted = model.predict(token_list)\n",
        "                predicted = np.argmax(predicted, axis=-1)\n",
        "                output_word = \"\"\n",
        "                for word, index in tokenizer.word_index.items():\n",
        "                    if index == predicted:\n",
        "                        output_word = word\n",
        "                        break\n",
        "                seed_text += \" \" + output_word\n",
        "                sentence += \" \" + output_word\n",
        "            if word_number < next_words:\n",
        "                paragraph += sentence.strip().capitalize() + \"\\n\"\n",
        "            seed_text = output_word\n",
        "        seed_text = sentence\n",
        "        poem += paragraph + \"\\n\"\n",
        "        paragraphs -= 1\n",
        "\n",
        "    print(poem)\n",
        "    return poem"
      ],
      "metadata": {
        "id": "HC-hzzcQ1lDn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting and preprocessing a text corpus\n",
        "text, average_words_number = get_corpus(url=\"https://www.gutenberg.org/cache/epub/45470/pg45470.txt\", get_part=True, start_phrase=\"THE SHINING HOURS\",\n",
        "                    end_phrase=\"End of the Project Gutenberg EBook\" )\n"
      ],
      "metadata": {
        "id": "w05bIoQy1nt-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tguCISF4HD-",
        "outputId": "129dc550-6597-4c5c-fb02-b327c7b69cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeffThe Project Gutenberg EBook of The Love Poems, by Ã‰mile Verhaeren',\n",
              " '',\n",
              " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
              " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
              " 're-use it under the terms of the Project Gutenberg License included',\n",
              " 'with this eBook or online at www.gutenberg.org/license',\n",
              " '',\n",
              " '',\n",
              " 'Title: The Love Poems',\n",
              " \"(From Les Heures claires, Les Heures d'aprÃ¨s-midi, Les Heures du Soir)\"]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_words_number"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHS1KTcM4loI",
        "outputId": "bf1e2854-6de6-4598-96cc-7a29f13c709a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the extracted text\n",
        "tokenizer, vocabulary_length =  create_tokenizer(text)\n"
      ],
      "metadata": {
        "id": "Q7kUEzS53ioO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cCLeAvn5yAg",
        "outputId": "c950f989-0e14-4812-f6b4-7fb0a7f67425"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad text sequences\n",
        "sequence_length, predictors, labels = pack_sequences(text, tokenizer, vocabulary_length)\n"
      ],
      "metadata": {
        "id": "8vjZ_4RM3krG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdATnPlY9iDC",
        "outputId": "c8b6ef0f-2577-4420-a448-9dc0933d13e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8YWipbD9O_R",
        "outputId": "f0255b55-2360-4cf7-bf2f-4a42628ae973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create and the poem generating model\n",
        "poems = create_model(vocabulary_length, sequence_length)\n"
      ],
      "metadata": {
        "id": "1TMfvzVO3m-d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model summary\n",
        "print(poems.summary())\n",
        "\n"
      ],
      "metadata": {
        "id": "L_lOk3xu3p4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7a5877-3929-459a-9944-dce6b3115429"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 14, 100)           371400    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 300)              301200    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3714)              1117914   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,790,514\n",
            "Trainable params: 1,790,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit compiled model\n",
        "history = poems.fit(predictors, labels, epochs=50, verbose=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "GTv8iJCq3rJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c873fe-4ce7-4b67-8bb4-15e314d15369"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "451/451 [==============================] - 33s 65ms/step - loss: 6.7652 - accuracy: 0.0704\n",
            "Epoch 2/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 6.1339 - accuracy: 0.0888\n",
            "Epoch 3/50\n",
            "451/451 [==============================] - 30s 65ms/step - loss: 5.7754 - accuracy: 0.1132\n",
            "Epoch 4/50\n",
            "451/451 [==============================] - 30s 66ms/step - loss: 5.4609 - accuracy: 0.1272\n",
            "Epoch 5/50\n",
            "451/451 [==============================] - 30s 66ms/step - loss: 5.1320 - accuracy: 0.1455\n",
            "Epoch 6/50\n",
            "451/451 [==============================] - 30s 65ms/step - loss: 4.7708 - accuracy: 0.1652\n",
            "Epoch 7/50\n",
            "451/451 [==============================] - 30s 66ms/step - loss: 4.3766 - accuracy: 0.1891\n",
            "Epoch 8/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 3.9902 - accuracy: 0.2189\n",
            "Epoch 9/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 3.6236 - accuracy: 0.2568\n",
            "Epoch 10/50\n",
            "451/451 [==============================] - 30s 66ms/step - loss: 3.2685 - accuracy: 0.3083\n",
            "Epoch 11/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 2.9425 - accuracy: 0.3674\n",
            "Epoch 12/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 2.6522 - accuracy: 0.4245\n",
            "Epoch 13/50\n",
            "451/451 [==============================] - 30s 66ms/step - loss: 2.3721 - accuracy: 0.4784\n",
            "Epoch 14/50\n",
            "451/451 [==============================] - 31s 68ms/step - loss: 2.1204 - accuracy: 0.5354\n",
            "Epoch 15/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 1.9126 - accuracy: 0.5802\n",
            "Epoch 16/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 1.7236 - accuracy: 0.6223\n",
            "Epoch 17/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 1.5503 - accuracy: 0.6591\n",
            "Epoch 18/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 1.3941 - accuracy: 0.6907\n",
            "Epoch 19/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 1.2623 - accuracy: 0.7265\n",
            "Epoch 20/50\n",
            "451/451 [==============================] - 30s 66ms/step - loss: 1.1429 - accuracy: 0.7481\n",
            "Epoch 21/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 1.0339 - accuracy: 0.7757\n",
            "Epoch 22/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.9419 - accuracy: 0.8004\n",
            "Epoch 23/50\n",
            "451/451 [==============================] - 30s 66ms/step - loss: 0.8593 - accuracy: 0.8150\n",
            "Epoch 24/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.7880 - accuracy: 0.8314\n",
            "Epoch 25/50\n",
            "451/451 [==============================] - 30s 65ms/step - loss: 0.7311 - accuracy: 0.8424\n",
            "Epoch 26/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.6655 - accuracy: 0.8553\n",
            "Epoch 27/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.6181 - accuracy: 0.8632\n",
            "Epoch 28/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.5826 - accuracy: 0.8711\n",
            "Epoch 29/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.5434 - accuracy: 0.8773\n",
            "Epoch 30/50\n",
            "451/451 [==============================] - 31s 68ms/step - loss: 0.5135 - accuracy: 0.8833\n",
            "Epoch 31/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.4841 - accuracy: 0.8878\n",
            "Epoch 32/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.4687 - accuracy: 0.8893\n",
            "Epoch 33/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.4509 - accuracy: 0.8920\n",
            "Epoch 34/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.4390 - accuracy: 0.8925\n",
            "Epoch 35/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.4269 - accuracy: 0.8954\n",
            "Epoch 36/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.4038 - accuracy: 0.8985\n",
            "Epoch 37/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.3939 - accuracy: 0.8995\n",
            "Epoch 38/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.3867 - accuracy: 0.8989\n",
            "Epoch 39/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.3814 - accuracy: 0.9009\n",
            "Epoch 40/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.3722 - accuracy: 0.9008\n",
            "Epoch 41/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.3684 - accuracy: 0.9005\n",
            "Epoch 42/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.3620 - accuracy: 0.9020\n",
            "Epoch 43/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.3600 - accuracy: 0.9010\n",
            "Epoch 44/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.3563 - accuracy: 0.9013\n",
            "Epoch 45/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.3484 - accuracy: 0.9035\n",
            "Epoch 46/50\n",
            "451/451 [==============================] - 30s 67ms/step - loss: 0.3443 - accuracy: 0.9030\n",
            "Epoch 47/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.3438 - accuracy: 0.9040\n",
            "Epoch 48/50\n",
            "451/451 [==============================] - 29s 65ms/step - loss: 0.3400 - accuracy: 0.9024\n",
            "Epoch 49/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.3356 - accuracy: 0.9040\n",
            "Epoch 50/50\n",
            "451/451 [==============================] - 29s 64ms/step - loss: 0.3312 - accuracy: 0.9049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poems.save(\"poems\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZzFl8LY3zw4",
        "outputId": "37c5869b-9a4c-4f79-c2e0-4983dbe63cc3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: poems/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: poems/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff92539d610> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff925326390> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !zip -r poems.zip poems"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84pZEJ-U4EGX",
        "outputId": "bd15d5bc-da8c-49a7-ec46-bbd6ae796d71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: poems/ (stored 0%)\n",
            "  adding: poems/assets/ (stored 0%)\n",
            "  adding: poems/keras_metadata.pb (deflated 89%)\n",
            "  adding: poems/variables/ (stored 0%)\n",
            "  adding: poems/variables/variables.index (deflated 64%)\n",
            "  adding: poems/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: poems/saved_model.pb (deflated 91%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate poetry\n",
        "write_poem(poems, tokenizer, 15, seed_text=\"Shine in the darkness\", next_words=5, paragraphs=3)\n"
      ],
      "metadata": {
        "id": "ca2Ls2ND3sih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "25b4cb9b-c46f-4479-f3bf-fd73b3915bf5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shine in the darkness\n",
            "\n",
            "At the fall of evening,\n",
            "I part your hair, and\n",
            "I make towards you, happy\n",
            "And serene, they believe eagerly;\n",
            "Its offering, my joy and\n",
            "\n",
            "The fervour of my flesh.\n",
            "Oh! how everything, except that\n",
            "Lives in the fine ruddy\n",
            "Being seems to dwell in\n",
            "The summer wind, this page\n",
            "\n",
            "And that so so open\n",
            "Forth in the general terms\n",
            "Of this agreement, you may\n",
            "My two hands against your\n",
            "Eyes were then so pure\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Shine in the darkness\\n\\nAt the fall of evening,\\nI part your hair, and\\nI make towards you, happy\\nAnd serene, they believe eagerly;\\nIts offering, my joy and\\n\\nThe fervour of my flesh.\\nOh! how everything, except that\\nLives in the fine ruddy\\nBeing seems to dwell in\\nThe summer wind, this page\\n\\nAnd that so so open\\nForth in the general terms\\nOf this agreement, you may\\nMy two hands against your\\nEyes were then so pure\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}
